{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-02T09:06:18.688097Z",
     "iopub.status.busy": "2025-02-02T09:06:18.687777Z",
     "iopub.status.idle": "2025-02-02T09:07:03.407594Z",
     "shell.execute_reply": "2025-02-02T09:07:03.406662Z",
     "shell.execute_reply.started": "2025-02-02T09:06:18.688066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T09:07:03.408969Z",
     "iopub.status.busy": "2025-02-02T09:07:03.408713Z",
     "iopub.status.idle": "2025-02-02T09:07:06.584495Z",
     "shell.execute_reply": "2025-02-02T09:07:06.583509Z",
     "shell.execute_reply.started": "2025-02-02T09:07:03.408946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T09:07:06.585849Z",
     "iopub.status.busy": "2025-02-02T09:07:06.585541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from transformers import pipeline\n",
    "\n",
    "# 로깅 설정 (에러 레벨에서만 로그 출력)\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 체크포인트 디렉터리 설정\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints/kafka_to_mongo\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Kafka 및 MongoDB 설정\n",
    "config = {\n",
    "    \"kafka\": {\n",
    "        \"bootstrap.servers\": os.getenv(\"BOOTSTRAP_SERVERS\"),\n",
    "        \"security.protocol\": \"SASL_SSL\",\n",
    "        \"sasl.mechanisms\": \"PLAIN\",\n",
    "        \"sasl.username\": os.getenv(\"SASL_USERNAME\"),\n",
    "        \"sasl.password\": os.getenv(\"SASL_PASSWORD\"),\n",
    "        \"client.id\": \"json-serial-producer\"\n",
    "    },\n",
    "    \"mongodb\": {\n",
    "        \"uri\": os.getenv(\"URI\"),\n",
    "        \"database\": os.getenv(\"DATABASE\"),\n",
    "        \"collection\": os.getenv(\"COLLECTION\")\n",
    "    }\n",
    "}\n",
    "\n",
    "# Kafka 토픽 설정\n",
    "topic = \"raw_topic\"\n",
    "\n",
    "# 감정 분석 모델 로드\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    텍스트 데이터에 대한 감정 분석 수행\n",
    "    :param text: 분석할 텍스트 (문자열)\n",
    "    :return: 긍정(positive) 또는 부정(negative) 라벨 반환\n",
    "    \"\"\"\n",
    "    if text and isinstance(text, str):\n",
    "        try:\n",
    "            result = sentiment_pipeline(text)[0]\n",
    "            return result['label']\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in sentiment analysis: {e}\")\n",
    "            return \"Error\"\n",
    "    return \"Empty or Invalid\"\n",
    "\n",
    "# 감정 분석 UDF\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "def read_from_kafka_and_write_to_mongo(spark):\n",
    "    \"\"\"\n",
    "    Kafka에서 스트리밍 데이터를 읽고 감정 분석을 수행한 후 MongoDB에 저장\n",
    "    :param spark: SparkSession 객체\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"review_id\", StringType()),\n",
    "        StructField(\"user_id\", StringType()),\n",
    "        StructField(\"business_id\", StringType()),\n",
    "        StructField(\"stars\", FloatType()),\n",
    "        StructField(\"useful\", IntegerType()),\n",
    "        StructField(\"funny\", IntegerType()),\n",
    "        StructField(\"cool\", IntegerType()),\n",
    "        StructField(\"text\", StringType()),\n",
    "        StructField(\"date\", StringType())\n",
    "    ])\n",
    "    \n",
    "    # Kafka 스트리밍 데이터 읽기\n",
    "    stream_df = (\n",
    "        spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", config['kafka']['bootstrap.servers'])\n",
    "            .option(\"subscribe\", topic)\n",
    "            .option(\"kafka.security.protocol\", config['kafka']['security.protocol'])\n",
    "            .option(\"kafka.sasl.mechanism\", config['kafka']['sasl.mechanisms'])\n",
    "            .option(\n",
    "                \"kafka.sasl.jaas.config\",\n",
    "                f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{config[\"kafka\"][\"sasl.username\"]}\" '\n",
    "                f'password=\"{config[\"kafka\"][\"sasl.password\"]}\";'\n",
    "            )\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .load()\n",
    "    )\n",
    "    \n",
    "    # Kafka 메시지에서 JSON 데이터 변환\n",
    "    parsed_df = stream_df.select(from_json(col('value').cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "    \n",
    "    # 감정 분석 컬럼 추가\n",
    "    enriched_df = parsed_df.withColumn(\"sentiment\", sentiment_udf(col('text')))\n",
    "    \n",
    "    # MongoDB로 스트리밍 데이터 저장\n",
    "    query = (\n",
    "        enriched_df.writeStream\n",
    "            .format(\"mongodb\")\n",
    "            .option(\"spark.mongodb.connection.uri\", config['mongodb']['uri'])\n",
    "            .option(\"spark.mongodb.database\", config['mongodb']['database'])\n",
    "            .option(\"spark.mongodb.collection\", config['mongodb']['collection'])\n",
    "            .option(\"checkpointLocation\", checkpoint_dir)\n",
    "            .outputMode(\"append\")\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    "    )\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # Spark 세션 생성\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "            .appName(\"KafkaStreamToMongo\")\n",
    "            .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n",
    "            .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    # Kafka에서 데이터를 읽고 MongoDB로 저장하는 함수\n",
    "    read_from_kafka_and_write_to_mongo(spark)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
